name: CI/CD Pipeline for Node.js API and PostgreSQL
on:
  push:
    branches:
      - master

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code from the repository
        uses: actions/checkout@v2

      - name: Set up SSH key for EC2
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.AWS_EC2_SSH_KEY }}" | base64 --decode > ~/.ssh/test-initial.pem
          chmod 600 ~/.ssh/test-initial.pem  # Set correct permissions for the key
          ls -l ~/.ssh  # Check permissions and verify file

      - name: Add EC2 to known hosts
        run: |
          ssh-keyscan -H ${{ secrets.AWS_EC2_HOST }} >> ~/.ssh/known_hosts

      - name: Add SSH Key to Agent
        run: |
          eval $(ssh-agent -s)  # Start the SSH agent
          ssh-add ~/.ssh/test-initial.pem  # Add the private key to the agent

      - name: SSH into EC2 and deploy
        run: |
          echo "Running SSH command..."
          set -x  # Enable shell debugging

          ssh -vvv -i ~/.ssh/test-initial.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ec2-user@${{ secrets.AWS_EC2_HOST }} <<'EOF'
            echo "SSH connected to EC2 instance"
            
            # Navigate to the project directory
            cd /home/ec2-user/node-test || exit
            ls -l

            # Export secrets as environment variables
            export POSTGRES_USER=${{ secrets.AWS_POSTGRES_USER }}
            export POSTGRES_PASSWORD=${{ secrets.AWS_POSTGRES_PASSWORD }}
            export POSTGRES_DB=${{ secrets.AWS_POSTGRES_DB }}
            export DATABASE_URL=${{ secrets.AWS_DATABASE_URL }}
            export AUTH_CHECK_STRING=${{ secrets.AUTH_CHECK_STRING }}
            export MAILER_LITE_API_TOKEN=${{ secrets.MAILER_LITE_API_TOKEN }}
            export AWS_COGNITO_REGION=${{ secrets.AWS_COGNITO_REGION }}
            export AWS_COGNITO_USER_POOL_ID=${{ secrets.AWS_COGNITO_USER_POOL_ID }}
            export AWS_COGNITO_CLIENT_ID=${{ secrets.AWS_COGNITO_CLIENT_ID }}
            export AWS_USER_ACCESS_KEY=${{ secrets.AWS_USER_ACCESS_KEY }}
            export AWS_USER_ACCESS_SECRET=${{ secrets.AWS_USER_ACCESS_SECRET }}

            # Step 1: Ensure Docker is installed on EC2
            if ! command -v docker &> /dev/null; then
                echo "Docker is not installed. Installing Docker..."
                sudo apt-get update
                sudo apt-get install -y docker.io
                sudo systemctl start docker
                sudo systemctl enable docker
            else
                echo "Docker is already installed."
            fi

            # Step 2: Ensure Docker Compose is installed
            if ! command -v docker-compose &> /dev/null; then
                echo "Docker Compose is not installed. Installing Docker Compose..."
                sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
                sudo chmod +x /usr/local/bin/docker-compose
            else
                echo "Docker Compose is already installed."
            fi

            # Step 2.1: Less disruptive cleanup strategy
            # This will stop and remove only the old, running production container.
            # It then prunes exited containers and unused images to free up space.
            echo "Stopping and removing the old production container..."
            docker-compose -f docker-compose.prod.yml down
            
            echo "Cleaning up all exited containers..."
            docker container prune -f

            echo "Removing all unused images..."
            docker image prune -a -f

            echo "System clean"
            docker system prune -a -f --volumes

            # Step 3: Pull the latest code from GitHub
            echo "Pulling latest code from GitHub"
            git fetch origin
            git checkout master
            git pull origin master

            # Step 4: Run tests using docker-compose.test.yml (without --abort-on-container-exit)
            echo "Running tests using docker-compose.test.yml"
            docker-compose -f docker-compose.test.yml up -d  # Run tests without aborting other containers

            # Get the container ID of the test container
            TEST_CONTAINER_ID=$(docker ps -q --filter "name=node-api-test")

            if [ -z "$TEST_CONTAINER_ID" ]; then
                echo "Test container not found, stopping deployment."
                exit 1  # Stop deployment if test container is not found
            fi

            # Get the exit code of the test container
              TEST_EXIT_CODE=$(docker inspect -f '{{.State.ExitCode}}' "$TEST_CONTAINER_ID")
              echo "Test container exit code: $TEST_EXIT_CODE"

              if [ "$TEST_EXIT_CODE" -eq 0 ]; then
                  echo "Tests passed, proceeding with deployment."
              else
                  echo "Tests failed, stopping deployment."
                  docker-compose -f docker-compose.test.yml down  # Clean up containers
                  exit 1  # Stop deployment if tests failed
              fi

            # Cleanup: Stop and remove the test container
            echo "Cleaning up test container..."
            docker-compose -f docker-compose.test.yml down

            # Rebuild the image only if necessary (pull new base images if available)
            docker-compose -f docker-compose.prod.yml build --pull

            # Remove dangling Docker images to clean up unused images
            docker image prune -f --filter "dangling=true"

            # Recreate and start containers, ensuring the latest image is used
            docker-compose -f docker-compose.prod.yml up -d --build

            # Step 5: Build the new Docker image and recreate the containers
            echo "Building the Docker image for production"
            docker-compose -f docker-compose.prod.yml build 

            # Step 5.1: Removing all dangling images
            echo "Remove dangling Docker images"
            docker image prune -f --filter "dangling=true"

            # Step 6: Run production containers using docker-compose.yml
            echo "Running docker-compose up for production containers"
            docker-compose -f docker-compose.prod.yml up -d

            # Wait for the Node.js app container to be fully started
            echo "Waiting for Node.js app container to start..."
            sleep 10  # Give the container time to initialize

            # Step 7: Run Prisma migrations inside the production container

            status_output=$(docker-compose exec hopeful_cohen bash -c "bun prisma migrate status")
            echo "$status_output"

            if echo "$status_output" | grep -q "up-to-date"; then
              echo "Migrations are up to date, skipping deployment."
            else
              echo "Applying migrations..."
              docker exec hopeful_cohen bash -c "bun prisma migrate deploy"
            fi

             # Step 8: Upload the Swagger JSON file to S3
            echo "Uploading Swagger JSON to S3..."
            aws s3 cp /home/ec2-user/node-test/src/swagger-doc.json s3://${{ secrets.AWS_BUCKET_NAME }}/documentation/swagger-doc.json


            echo "Deployment completed."
          EOF
